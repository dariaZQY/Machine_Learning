{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST distributed training  \n",
    "\n",
    "The **SageMaker Python SDK** helps you deploy your models for training and hosting in optimized, productions ready containers in SageMaker. The SageMaker Python SDK is easy to use, modular, extensible and compatible with TensorFlow and MXNet. This tutorial focuses on how to create a convolutional neural network model to train the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) using **TensorFlow distributed training**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "('Writing', 'data/train.tfrecords')\n",
      "('Writing', 'data/validation.tfrecords')\n",
      "('Writing', 'data/test.tfrecords')\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "data_sets = mnist.read_data_sets('data', dtype=tf.uint8, reshape=False, validation_size=5000)\n",
    "\n",
    "utils.convert_to(data_sets.train, 'train', 'data')\n",
    "utils.convert_to(data_sets.validation, 'validation', 'data')\n",
    "utils.convert_to(data_sets.test, 'test', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "We use the ```sagemaker.Session.upload_data``` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-928663186773\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for distributed training \n",
    "Here is the full code for the network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\r\n",
      "\r\n",
      "INPUT_TENSOR_NAME = 'inputs'\r\n",
      "SIGNATURE_NAME = 'predictions'\r\n",
      "\r\n",
      "LEARNING_RATE = 0.001\r\n",
      "\r\n",
      "\r\n",
      "def model_fn(features, labels, mode, params):\r\n",
      "    # Input Layer\r\n",
      "    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\r\n",
      "\r\n",
      "    # Convolutional Layer #1\r\n",
      "    conv1 = tf.layers.conv2d(\r\n",
      "        inputs=input_layer,\r\n",
      "        filters=32,\r\n",
      "        kernel_size=[5, 5],\r\n",
      "        padding='same',\r\n",
      "        activation=tf.nn.relu)\r\n",
      "\r\n",
      "    # Pooling Layer #1\r\n",
      "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\r\n",
      "\r\n",
      "    # Convolutional Layer #2 and Pooling Layer #2\r\n",
      "    conv2 = tf.layers.conv2d(\r\n",
      "        inputs=pool1,\r\n",
      "        filters=64,\r\n",
      "        kernel_size=[5, 5],\r\n",
      "        padding='same',\r\n",
      "        activation=tf.nn.relu)\r\n",
      "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\r\n",
      "\r\n",
      "    # Dense Layer\r\n",
      "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\r\n",
      "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\r\n",
      "    dropout = tf.layers.dropout(\r\n",
      "        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\r\n",
      "\r\n",
      "    # Logits Layer\r\n",
      "    logits = tf.layers.dense(inputs=dropout, units=10)\r\n",
      "\r\n",
      "    # Define operations\r\n",
      "    if mode in (Modes.PREDICT, Modes.EVAL):\r\n",
      "        predicted_indices = tf.argmax(input=logits, axis=1)\r\n",
      "        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\r\n",
      "\r\n",
      "    if mode in (Modes.TRAIN, Modes.EVAL):\r\n",
      "        global_step = tf.train.get_or_create_global_step()\r\n",
      "        label_indices = tf.cast(labels, tf.int32)\r\n",
      "        loss = tf.losses.softmax_cross_entropy(\r\n",
      "            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\r\n",
      "        tf.summary.scalar('OptimizeLoss', loss)\r\n",
      "\r\n",
      "    if mode == Modes.PREDICT:\r\n",
      "        predictions = {\r\n",
      "            'classes': predicted_indices,\r\n",
      "            'probabilities': probabilities\r\n",
      "        }\r\n",
      "        export_outputs = {\r\n",
      "            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\r\n",
      "        }\r\n",
      "        return tf.estimator.EstimatorSpec(\r\n",
      "            mode, predictions=predictions, export_outputs=export_outputs)\r\n",
      "\r\n",
      "    if mode == Modes.TRAIN:\r\n",
      "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\r\n",
      "        train_op = optimizer.minimize(loss, global_step=global_step)\r\n",
      "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n",
      "\r\n",
      "    if mode == Modes.EVAL:\r\n",
      "        eval_metric_ops = {\r\n",
      "            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\r\n",
      "        }\r\n",
      "        return tf.estimator.EstimatorSpec(\r\n",
      "            mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n",
      "\r\n",
      "\r\n",
      "def serving_input_fn(params):\r\n",
      "    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\r\n",
      "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\r\n",
      "\r\n",
      "\r\n",
      "def read_and_decode(filename_queue):\r\n",
      "    reader = tf.TFRecordReader()\r\n",
      "    _, serialized_example = reader.read(filename_queue)\r\n",
      "\r\n",
      "    features = tf.parse_single_example(\r\n",
      "        serialized_example,\r\n",
      "        features={\r\n",
      "            'image_raw': tf.FixedLenFeature([], tf.string),\r\n",
      "            'label': tf.FixedLenFeature([], tf.int64),\r\n",
      "        })\r\n",
      "\r\n",
      "    image = tf.decode_raw(features['image_raw'], tf.uint8)\r\n",
      "    image.set_shape([784])\r\n",
      "    image = tf.cast(image, tf.float32) * (1. / 255)\r\n",
      "    label = tf.cast(features['label'], tf.int32)\r\n",
      "\r\n",
      "    return image, label\r\n",
      "\r\n",
      "\r\n",
      "def train_input_fn(training_dir, params):\r\n",
      "    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\r\n",
      "\r\n",
      "\r\n",
      "def eval_input_fn(training_dir, params):\r\n",
      "    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\r\n",
      "\r\n",
      "\r\n",
      "def _input_fn(training_dir, training_filename, batch_size=100):\r\n",
      "    test_file = os.path.join(training_dir, training_filename)\r\n",
      "    filename_queue = tf.train.string_input_producer([test_file])\r\n",
      "\r\n",
      "    image, label = read_and_decode(filename_queue)\r\n",
      "    images, labels = tf.train.batch(\r\n",
      "        [image, label], batch_size=batch_size,\r\n",
      "        capacity=1000 + 3 * batch_size)\r\n",
      "\r\n",
      "    return {INPUT_TENSOR_NAME: images}, labels\r\n"
     ]
    }
   ],
   "source": [
    "!cat 'mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script here is and adaptation of the [TensorFlow MNIST example](https://github.com/tensorflow/models/tree/master/official/mnist). It provides a ```model_fn(features, labels, mode)```, which is used for training, evaluation and inference. \n",
    "\n",
    "## A regular ```model_fn```\n",
    "\n",
    "A regular **```model_fn```** follows the pattern:\n",
    "1. [defines a neural network](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L96)\n",
    "- [applies the ```features``` in the neural network](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L178)\n",
    "- [if the ```mode``` is ```PREDICT```, returns the output from the neural network](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L186)\n",
    "- [calculates the loss function comparing the output with the ```labels```](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L188)\n",
    "- [creates an optimizer and minimizes the loss function to improve the neural network](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L193)\n",
    "- [returns the output, optimizer and loss function](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L205)\n",
    "\n",
    "## Writing a ```model_fn``` for distributed training\n",
    "When distributed training happens, the same neural network will be sent to the multiple training instances. Each instance will predict a batch of the dataset, calculate loss and minimize the optimizer. One entire loop of this process is called **training step**.\n",
    "\n",
    "### Syncronizing training steps\n",
    "A [global step](https://www.tensorflow.org/api_docs/python/tf/train/global_step) is a global variable shared between the instances. It necessary for distributed training, so the optimizer will keep track of the number of **training steps** between runs: \n",
    "\n",
    "```python\n",
    "train_op = optimizer.minimize(loss, tf.train.get_or_create_global_step())\n",
    "```\n",
    "\n",
    "That is the only required change for distributed training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.TensorFlow estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-928663186773\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-py2-cpu-2018-03-09-07-44-53-066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................\n",
      "\u001b[31mexecuting startup script (first run)\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:33,238 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:33,238 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:34,862 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:35,792 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:35,863 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:{\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"algo-2:2222\"], \"ps\": [\"algo-1:2223\", \"algo-2:2223\"], \"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}}\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:---------------------------------------------------------\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:going to training\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:35,913 INFO - root - creating RunConfig:\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:35,914 INFO - root - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:35.914147: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:35,914 INFO - root - creating the estimator\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Using config: {'_model_dir': u's3://sagemaker-us-east-1-928663186773/sagemaker-tensorflow-py2-cpu-2018-03-09-07-44-53-066/checkpoints', '_save_checkpoints_secs': 300, '_num_ps_replicas': 2, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f19ad7f4950>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31m, '_num_worker_replicas': 2, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://algo-1:2222', '_log_step_count_steps': 100}\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:35,915 INFO - root - creating Experiment:\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:35,915 INFO - root - {'min_eval_frequency': 1000}\u001b[0m\n",
      "\u001b[31mE0309 07:50:35.917591206      60 ev_epoll1_linux.c:1051]     grpc epoll fd: 5\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:35.924751: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> algo-1:2222}\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:35.924773: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2223, 1 -> algo-2:2223}\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:35.924782: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> algo-2:2222}\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:35.925196: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mMonitors are deprecated. Please use tf.train.SessionRunHook.\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:36.007525: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> localhost:2222}\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:36.007555: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> algo-1:2223, 1 -> algo-2:2223}\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:36.007565: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> algo-2:2222}\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:36.007740: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[32mexecuting startup script (first run)\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:36,312 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:36,312 INFO - root - starting train task\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:37,950 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-03-09 07:50:39.206836: I tensorflow/core/distributed_runtime/master_session.cc:1004] Start master session 51d54e4dddeb314c with config: gpu_options { per_process_gpu_memory_fraction: 1 } allow_soft_placement: true\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:38,970 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39,041 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:{\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"algo-2:2222\"], \"ps\": [\"algo-1:2223\", \"algo-2:2223\"], \"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:---------------------------------------------------------\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:going to training\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39,089 INFO - root - creating RunConfig:\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39,089 INFO - root - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39.089771: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39,090 INFO - root - creating the estimator\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:Using config: {'_model_dir': u's3://sagemaker-us-east-1-928663186773/sagemaker-tensorflow-py2-cpu-2018-03-09-07-44-53-066/checkpoints', '_save_checkpoints_secs': 300, '_num_ps_replicas': 2, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff1d81b7950>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32m, '_num_worker_replicas': 2, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://algo-2:2222', '_log_step_count_steps': 100}\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39,090 INFO - root - creating Experiment:\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39,090 INFO - root - {'min_eval_frequency': 1000}\u001b[0m\n",
      "\u001b[32mE0309 07:50:39.093128180       1 ev_epoll1_linux.c:1051]     grpc epoll fd: 5\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39.101197: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> algo-1:2222}\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39.101230: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> algo-1:2223, 1 -> algo-2:2223}\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39.101239: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222}\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39.101694: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> algo-1:2222}\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39.101718: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> algo-1:2223, 1 -> localhost:2223}\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39.101727: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> algo-2:2222}\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39.101820: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39.102170: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[32m2018-03-09 07:50:39.598455: I tensorflow/core/distributed_runtime/master_session.cc:1004] Start master session dad30fb4d4da0247 with config: gpu_options { per_process_gpu_memory_fraction: 1 } allow_soft_placement: true\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 2.3103585, step = 0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mINFO:tensorflow:Saving checkpoints for 4 into s3://sagemaker-us-east-1-928663186773/sagemaker-tensorflow-py2-cpu-2018-03-09-07-44-53-066/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.079696186, step = 101 (20.964 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 1.9906484, step = 1\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.54466\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.1243246, step = 255 (25.502 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.73075\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.014153212, step = 347 (30.746 sec)\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.034813765, step = 448 (28.553 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.79012\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.7051\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.021455158, step = 554 (30.694 sec)\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.04294903, step = 645 (29.848 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.50465\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.69112\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.025271274, step = 758 (30.938 sec)\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.006157181, step = 838 (28.654 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.79929\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.72524\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.025843883, step = 963 (30.514 sec)\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:Loss for final step: 0.09376046.\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:writing success training\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving checkpoints for 1001 into s3://sagemaker-us-east-1-928663186773/sagemaker-tensorflow-py2-cpu-2018-03-09-07-44-53-066/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Loss for final step: 0.015195616.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Starting evaluation at 2018-03-09-07:53:24\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-east-1-928663186773/sagemaker-tensorflow-py2-cpu-2018-03-09-07-44-53-066/checkpoints/model.ckpt-1001\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [1/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [2/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [3/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [4/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [5/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [6/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [7/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [8/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [9/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [10/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [11/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [12/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [13/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [14/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [15/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [16/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [17/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [18/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [19/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [20/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [21/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [22/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [23/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [24/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [25/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [26/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [27/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [28/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [29/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [30/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [31/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [32/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [33/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [34/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [35/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [36/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [37/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [38/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [39/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [40/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [41/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [42/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [43/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [44/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [45/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [46/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [47/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [48/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [49/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [50/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [51/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [52/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [53/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [54/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [55/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [56/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [57/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [58/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [59/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [60/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [61/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [62/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [63/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [64/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [65/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [66/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [67/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [68/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [69/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [70/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [71/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [72/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [73/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [74/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [75/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [76/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [77/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [78/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [79/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [80/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [81/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [82/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [83/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [84/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [85/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [86/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [87/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [88/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [89/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [90/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [91/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [92/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [93/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [94/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [95/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [96/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [97/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [98/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [99/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [100/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Finished evaluation at 2018-03-09-07:53:31\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving dict for global step 1002: accuracy = 0.9874, global_step = 1002, loss = 0.042607542\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-east-1-928663186773/sagemaker-tensorflow-py2-cpu-2018-03-09-07-44-53-066/checkpoints/model.ckpt-1001\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Assets added to graph.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:No assets to write.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:SavedModel written to: s3://sagemaker-us-east-1-928663186773/sagemaker-tensorflow-py2-cpu-2018-03-09-07-44-53-066/checkpoints/export/Servo/temp-1520582015/saved_model.pb\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:writing success training\u001b[0m\n",
      "\u001b[31m2018-03-09 07:53:43,103 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-03-09 07:53:43,195 INFO - tf_container.serve - Downloaded saved model at /opt/ml/model/export/Servo/1520582015/saved_model.pb\u001b[0m\n",
      "\u001b[31m2018-03-09 07:53:43,208 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): s3.amazonaws.com\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:master algo-1 is down, stopping parameter server\u001b[0m\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             role=role,\n",
    "                             training_steps=1000, \n",
    "                             evaluation_steps=100,\n",
    "                             train_instance_count=2,\n",
    "                             train_instance_type='ml.c4.xlarge')\n",
    "\n",
    "mnist_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **```fit```** method will create a training job in two **ml.c4.xlarge** instances. The logs above will show the instances doing training, evaluation, and incrementing the number of **training steps**. \n",
    "\n",
    "In the end of the training, the training job will generate a saved model for TF serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deploy the trained model to prepare for predictions\n",
    "\n",
    "The deploy() method creates an endpoint which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-tensorflow-py2-cpu-2018-03-09-07-44-53-066\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModel operation: Cannot create already existing model \"arn:aws:sagemaker:us-east-1:928663186773:model/sagemaker-tensorflow-py2-cpu-2018-03-09-07-44-53-066\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mClientError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0637d1434f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m mnist_predictor = mnist_estimator.deploy(initial_instance_count=1,\n\u001b[0;32m----> 2\u001b[0;31m                                              instance_type='ml.m4.xlarge')\n\u001b[0m",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/sagemaker/estimator.pyc\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, endpoint_name, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             endpoint_name=endpoint_name)\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/sagemaker/model.pyc\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, endpoint_name)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mcontainer_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_container_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mname_from_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mproduction_variant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduction_variant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendpoint_name\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/sagemaker/session.pyc\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(self, name, role, primary_container)\u001b[0m\n\u001b[1;32m    269\u001b[0m         self.sagemaker_client.create_model(ModelName=name,\n\u001b[1;32m    270\u001b[0m                                            \u001b[0mPrimaryContainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprimary_container\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                                            ExecutionRoleArn=role)\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/botocore/client.pyc\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    316\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/botocore/client.pyc\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModel operation: Cannot create already existing model \"arn:aws:sagemaker:us-east-1:928663186773:model/sagemaker-tensorflow-py2-cpu-2018-03-09-07-44-53-066\"."
     ]
    }
   ],
   "source": [
    "mnist_predictor = mnist_estimator.deploy(initial_instance_count=1,\n",
    "                                             instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "for i in range(10):\n",
    "    data = mnist.test.images[i].tolist()\n",
    "    tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[1, len(data)], dtype=tf.float32)\n",
    "    predict_response = mnist_predictor.predict(tensor_proto)\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = predict_response['outputs']['classes']['int64Val'][0]\n",
    "    print(\"prediction is {}\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-tensorflow-py2-cpu-2018-03-09-07-44-53-066\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(mnist_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
